{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af047755",
   "metadata": {},
   "source": [
    "#### KAMAMO LESLEY WANJIKU\n",
    "#### 8984971\n",
    "#### CSCN8020-26W-Sec1- Reinforcement Learning Programming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d0253b",
   "metadata": {},
   "source": [
    "### PROBLEM 1\n",
    "\n",
    "`Pick-and-Place Robot`:\n",
    "\n",
    "Consider using reinforcement learning to control the motion of a robot arm\n",
    "in a repetitive pick-and-place task. If we want to learn movements that are fast and smooth, the\n",
    "learning agent will have to control the motors directly and obtain feedback about the current positions\n",
    "and velocities of the mechanical linkages.\n",
    "Design the reinforcement learning problem as an MDP, define states, actions, rewards with reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7b782b",
   "metadata": {},
   "source": [
    "The pick-and-place robot arm is modeled as an MDP where the state includes joint angles and task context (contact state), ensuring the Markov property.\n",
    "We model the task as an MDP:\n",
    "(S,A,P,R,Œ≥)\n",
    "\n",
    "At each discrete control step t(e.g., every 20‚Äì50 ms), the agent observes state s_t, chooses action a_t, the robot transitions to s_(t+1), and receives reward r_(t+1)\n",
    "The actions are continuous motor commands (joint torques, plus gripper open/close), matching low-level control for smooth motion. Transitions follow robot dynamics with possible stochasticity from noise and friction. The reward combines a large success reward for placing the object, a per-step time penalty to encourage speed, and smoothness/energy penalties on torque magnitude and torque changes to reduce jerk and wear, optionally shaped by distances to the object/goal to improve learning. A high discount factor (e.g., Œ≥=0.99) values completing the full task sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a665ac",
   "metadata": {},
   "source": [
    "### PROBLEM 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc197fd7",
   "metadata": {},
   "source": [
    "`Tasks`\n",
    "\n",
    "Perform two iterations of Value Iteration for this gridworld environment. Show the step-by-step process\n",
    "(without code) including policy evaluation and policy improvement. Provide the following for each\n",
    "iteration:\n",
    "\n",
    "`‚Ä¢ Iteration 1:`\n",
    "\n",
    "1. Show the initial value function (V) for each state.\n",
    "\n",
    "2. Perform value function updates.\n",
    "\n",
    "3. Show the updated value function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baa85c4",
   "metadata": {},
   "source": [
    "1. Initial Value Function\n",
    "\n",
    "[\n",
    "V_0(s_1)=0,\\quad\n",
    "V_0(s_2)=0,\\quad\n",
    "V_0(s_3)=0,\\quad\n",
    "V_0(s_4)=0\n",
    "]\n",
    "\n",
    "\n",
    "2. Policy Evaluation Step (Using Initial Policy œÄ‚ÇÄ)\n",
    "\n",
    "Since\n",
    "[\n",
    "\\pi_0(\\text{up}|s)=1\n",
    "]\n",
    "we evaluate:\n",
    "\n",
    "[\n",
    "V_1(s)=R(s)+\\gamma V_0(s')\n",
    "]\n",
    "\n",
    "where (s') is the next state when taking action **up**.\n",
    "\n",
    "Because (V_0(s)=0) for all states:\n",
    "\n",
    "* For (s_1): up hits wall ‚Üí stays in (s_1)\n",
    "  [\n",
    "  V_1(s_1)=5+0.9(0)=5\n",
    "  ]\n",
    "\n",
    "* For (s_2): up hits wall ‚Üí stays in (s_2)\n",
    "  [\n",
    "  V_1(s_2)=10+0.9(0)=10\n",
    "  ]\n",
    "\n",
    "* For (s_3): up ‚Üí (s_1)\n",
    "  [\n",
    "  V_1(s_3)=1+0.9(0)=1\n",
    "  ]\n",
    "\n",
    "* For (s_4): up ‚Üí (s_2)\n",
    "  [\n",
    "  V_1(s_4)=2+0.9(0)=2\n",
    "  ]\n",
    "\n",
    "_Updated Value Function after Iteration 1:_\n",
    "\n",
    "[\n",
    "V_1=\n",
    "\\begin{cases}\n",
    "s_1=5 \\\n",
    "s_2=10 \\\n",
    "s_3=1 \\\n",
    "s_4=2\n",
    "\\end{cases}\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "3. Policy Improvement Step\n",
    "\n",
    "Now we improve the policy by choosing the action that maximizes:\n",
    "\n",
    "[\n",
    "Q(s,a)=R(s)+\\gamma V_1(s')\n",
    "]\n",
    "\n",
    "\n",
    "**State (s_1)**\n",
    "\n",
    "* up: (5+0.9(5)=9.5)\n",
    "* left: (5+0.9(5)=9.5)\n",
    "* right: (5+0.9(10)=14)\n",
    "* down: (5+0.9(1)=5.9)\n",
    "\n",
    "Best action: **right**\n",
    "\n",
    "\n",
    "**State (s_2)**\n",
    "\n",
    "* up/right: (10+0.9(10)=19)\n",
    "* left: (10+0.9(5)=14.5)\n",
    "* down: (10+0.9(2)=11.8)\n",
    "\n",
    "Best action: **up or right**\n",
    "\n",
    "\n",
    "**State (s_3)**\n",
    "\n",
    "* down/left: (1+0.9(1)=1.9)\n",
    "* up: (1+0.9(5)=5.5)\n",
    "* right: (1+0.9(2)=2.8)\n",
    "\n",
    "Best action: **up**\n",
    "\n",
    "\n",
    "**State (s_4)**\n",
    "\n",
    "* down/right: (2+0.9(2)=3.8)\n",
    "* up: (2+0.9(10)=11)\n",
    "* left: (2+0.9(1)=2.9)\n",
    "\n",
    "Best action: **up**\n",
    "\n",
    "\n",
    "**_Improved Policy after Iteration 1_**\n",
    "\n",
    "[\n",
    "\\pi_1:\n",
    "\\begin{cases}\n",
    "s_1 \\rightarrow \\text{right} \\\n",
    "s_2 \\rightarrow \\text{up/right} \\\n",
    "s_3 \\rightarrow \\text{up} \\\n",
    "s_4 \\rightarrow \\text{up}\n",
    "\\end{cases}\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bc9d0b",
   "metadata": {},
   "source": [
    "`‚Ä¢ Iteration 2:` Show the value function (V) after the second iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f841019",
   "metadata": {},
   "source": [
    "\n",
    "We now compute a second value update using value iteration.\n",
    "\n",
    " 1. Policy Evaluation Step\n",
    "\n",
    "Using value iteration update rule:\n",
    "\n",
    "[\n",
    "V_2(s)=\\max_a \\big(R(s)+\\gamma V_1(s')\\big)\n",
    "]\n",
    "\n",
    "We use the values from (V_1).\n",
    "\n",
    "\n",
    "  **(s_1)**\n",
    "\n",
    "Maximum from previous calculations = **14**\n",
    "\n",
    "[\n",
    "V_2(s_1)=14\n",
    "]\n",
    "\n",
    "\n",
    "**(s_2)**\n",
    "\n",
    "Maximum from previous calculations = **19**\n",
    "\n",
    "[\n",
    "V_2(s_2)=19\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "**(s_3)**\n",
    "\n",
    "Maximum from previous calculations = **5.5**\n",
    "\n",
    "[\n",
    "V_2(s_3)=5.5\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "**(s_4)**\n",
    "\n",
    "Maximum from previous calculations = **11**\n",
    "\n",
    "[\n",
    "V_2(s_4)=11\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "  2. Value Function after Iteration 2\n",
    "\n",
    "[\n",
    "V_2=\n",
    "\\begin{cases}\n",
    "s_1=14 \\\n",
    "s_2=19 \\\n",
    "s_3=5.5 \\\n",
    "s_4=11\n",
    "\\end{cases}\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "  3. Policy Improvement Step (Iteration 2)\n",
    "\n",
    "The maximizing actions remain:\n",
    "\n",
    "[\n",
    "\\pi_2=\n",
    "\\begin{cases}\n",
    "s_1 \\rightarrow \\text{right} \\\n",
    "s_2 \\rightarrow \\text{up/right} \\\n",
    "s_3 \\rightarrow \\text{up} \\\n",
    "s_4 \\rightarrow \\text{up}\n",
    "\\end{cases}\n",
    "]\n",
    "\n",
    "No policy change from Iteration 1, so the policy is stabilizing.\n",
    "\n",
    "\n",
    "**After Iteration 1:**\n",
    "\n",
    "[\n",
    "V_1 = {5,10,1,2}\n",
    "]\n",
    "\n",
    "**After Iteration 2:**\n",
    "\n",
    "[\n",
    "V_2 = {14,19,5.5,11}\n",
    "]\n",
    "\n",
    "**_Improved Policy:_**\n",
    "\n",
    "* (s_1): right\n",
    "* (s_2): up or right\n",
    "* (s_3): up\n",
    "* (s_4): up\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f698b3f2",
   "metadata": {},
   "source": [
    "`Reflection`\n",
    "\n",
    "This Iteration 1 process shows how value iteration begins by evaluating the environment using a simple starting policy and then immediately improves it using the information gained. Because all initial state values are set to zero ((V_0=0)), the policy evaluation step under (\\pi_0(\\text{up}|s)=1) makes the updated values (V_1(s)) equal to the immediate rewards in each state (since the discounted future term (\\gamma V_0(s')) contributes nothing yet). After obtaining these first value estimates, the policy improvement step looks one move ahead and selects the action in each state that leads to the highest discounted value of the next state, which naturally pushes the agent toward higher-reward states like (s_2). As a result, the improved policy (\\pi_1) shifts away from always going ‚Äúup‚Äù and instead recommends moving right from (s_1), staying up/right in (s_2), and moving up from (s_3) and (s_4), demonstrating the core idea that value iteration uses value estimates to gradually refine the policy toward better long-term outcomes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66556abb",
   "metadata": {},
   "source": [
    "### PROBLEM 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fa2b9d",
   "metadata": {},
   "source": [
    "`Task1: Update MDP Code`\n",
    "\n",
    "1. Update the reward function to be a list of reward based on whether the state is terminal, grey,\n",
    "or a regular state.\n",
    "\n",
    "2. Run the existing code developed in class and obtain the optimal state-values and optimal policy.\n",
    "Provide a figures of the gridworld with the obtained V‚àó and œÄ‚àó (You can manually create a table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9228eb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826aec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "State = Tuple[int, int]\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Gridworld5x5:\n",
    "    n_rows: int = 5\n",
    "    n_cols: int = 5\n",
    "    goal: State = (4, 4)\n",
    "    grey_states: Tuple[State, ...] = ((1, 2), (3, 0), (0, 4))\n",
    "\n",
    "    # (name, (dr, dc))\n",
    "    actions: Tuple[Tuple[str, Tuple[int, int]], ...] = (\n",
    "        (\"R\", (0, 1)),\n",
    "        (\"D\", (1, 0)),\n",
    "        (\"L\", (0, -1)),\n",
    "        (\"U\", (-1, 0)),\n",
    "    )\n",
    "\n",
    "    def all_states(self) -> List[State]:\n",
    "        return [(r, c) for r in range(self.n_rows) for c in range(self.n_cols)]\n",
    "\n",
    "    def is_terminal(self, s: State) -> bool:\n",
    "        return s == self.goal\n",
    "\n",
    "    def reward(self, s: State) -> float:\n",
    "        \"\"\"Reward depends on the current state s .\"\"\"\n",
    "        if s == self.goal:\n",
    "            return 10.0\n",
    "        if s in self.grey_states:\n",
    "            return -5.0\n",
    "        return -1.0\n",
    "\n",
    "    def step(self, s: State, action_name: str) -> State:\n",
    "        \"\"\"Deterministic transition with wall-bounce (stay in place).\"\"\"\n",
    "        if self.is_terminal(s):\n",
    "            return s  # terminal state is absorbing for planning\n",
    "\n",
    "        # find action delta\n",
    "        delta = None\n",
    "        for name, d in self.actions:\n",
    "            if name == action_name:\n",
    "                delta = d\n",
    "                break\n",
    "        if delta is None:\n",
    "            raise ValueError(f\"Unknown action: {action_name}\")\n",
    "\n",
    "        r, c = s\n",
    "        dr, dc = delta\n",
    "        nr, nc = r + dr, c + dc\n",
    "\n",
    "        # wall -> stay\n",
    "        if nr < 0 or nr >= self.n_rows or nc < 0 or nc >= self.n_cols:\n",
    "            return s\n",
    "        return (nr, nc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0441fbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(\n",
    "    env: Gridworld5x5,\n",
    "    gamma: float = 0.9,\n",
    "    theta: float = 1e-6,\n",
    "    max_iters: int = 10000,\n",
    ") -> Tuple[np.ndarray, np.ndarray, int, float]:\n",
    "   \n",
    "    start = time.perf_counter()\n",
    "\n",
    "    V = np.zeros((env.n_rows, env.n_cols), dtype=float)\n",
    "    V_new = np.zeros_like(V)\n",
    "\n",
    "    # Map action index to name for policy storage/printing\n",
    "    action_names = [name for name, _ in env.actions]\n",
    "\n",
    "    it = 0\n",
    "    for it in range(1, max_iters + 1):\n",
    "        delta = 0.0\n",
    "\n",
    "        for r in range(env.n_rows):\n",
    "            for c in range(env.n_cols):\n",
    "                s = (r, c)\n",
    "\n",
    "                # Terminal: value can be set to its reward (or 0); assignment reward is +10 at goal.\n",
    "                # We'll set V(goal) = 10 to reflect R(s_goal)=+10 clearly in the table.\n",
    "                if env.is_terminal(s):\n",
    "                    V_new[r, c] = env.reward(s)\n",
    "                    continue\n",
    "\n",
    "                # Bellman optimality backup\n",
    "                best = -float(\"inf\")\n",
    "                for a in action_names:\n",
    "                    s_next = env.step(s, a)\n",
    "                    rr = env.reward(s)  # reward depends on current state\n",
    "                    candidate = rr + gamma * V[s_next[0], s_next[1]]\n",
    "                    if candidate > best:\n",
    "                        best = candidate\n",
    "\n",
    "                V_new[r, c] = best\n",
    "                delta = max(delta, abs(V_new[r, c] - V[r, c]))\n",
    "\n",
    "        V[:, :] = V_new  # copy new -> old for next iteration\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    # Derive greedy policy œÄ* from V*\n",
    "    policy_idx = np.full((env.n_rows, env.n_cols), fill_value=-1, dtype=int)\n",
    "\n",
    "    for r in range(env.n_rows):\n",
    "        for c in range(env.n_cols):\n",
    "            s = (r, c)\n",
    "            if env.is_terminal(s):\n",
    "                policy_idx[r, c] = -1\n",
    "                continue\n",
    "\n",
    "            best_a = 0\n",
    "            best_val = -float(\"inf\")\n",
    "            for i, a in enumerate(action_names):\n",
    "                s_next = env.step(s, a)\n",
    "                rr = env.reward(s)\n",
    "                val = rr + gamma * V[s_next[0], s_next[1]]\n",
    "                if val > best_val:\n",
    "                    best_val = val\n",
    "                    best_a = i\n",
    "\n",
    "            policy_idx[r, c] = best_a\n",
    "\n",
    "    elapsed = time.perf_counter() - start\n",
    "    return V, policy_idx, it, elapsed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee378536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_policy(env: Gridworld5x5, policy_idx: np.ndarray) -> List[List[str]]:\n",
    "    \"\"\"Turn action indices into arrows + mark special states.\"\"\"\n",
    "    idx_to_arrow = {0: \"‚Üí\", 1: \"‚Üì\", 2: \"‚Üê\", 3: \"‚Üë\", -1: \"G\"}  # -1 only used for goal\n",
    "    grid = []\n",
    "    for r in range(env.n_rows):\n",
    "        row = []\n",
    "        for c in range(env.n_cols):\n",
    "            s = (r, c)\n",
    "            if env.is_terminal(s):\n",
    "                row.append(\"G\")\n",
    "            elif s in env.grey_states:\n",
    "                # still show policy arrow but visually tag it as grey\n",
    "                row.append(f\"X{idx_to_arrow[policy_idx[r, c]]}\")\n",
    "            else:\n",
    "                row.append(idx_to_arrow[policy_idx[r, c]])\n",
    "        grid.append(row)\n",
    "    return grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "904929d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Value Iteration Results ===\n",
      "Iterations: 10\n",
      "Optimization time: 0.001811 seconds\n",
      "\n",
      "V* (state-value function):\n",
      "[[-1.39 -0.43  0.63  1.81 -0.88]\n",
      " [-0.43  0.63 -2.19  3.12  4.58]\n",
      " [ 0.63  1.81  3.12  4.58  6.2 ]\n",
      " [-2.19  3.12  4.58  6.2   8.  ]\n",
      " [ 3.12  4.58  6.2   8.   10.  ]]\n",
      "\n",
      "œÄ* (greedy policy from V*):\n",
      " ‚Üí  ‚Üí  ‚Üí  ‚Üì X‚Üì\n",
      " ‚Üí  ‚Üì X‚Üí  ‚Üí  ‚Üì\n",
      " ‚Üí  ‚Üí  ‚Üí  ‚Üí  ‚Üì\n",
      "X‚Üí  ‚Üí  ‚Üí  ‚Üí  ‚Üì\n",
      " ‚Üí  ‚Üí  ‚Üí  ‚Üí  G\n",
      "\n",
      "Legend:\n",
      "  ‚Üí Right, ‚Üì Down, ‚Üê Left, ‚Üë Up, G Goal/Terminal, X? = Grey state with suggested move\n"
     ]
    }
   ],
   "source": [
    "def print_results(env, V, policy_idx, iters, elapsed, label=\"Value Iteration Results\"):\n",
    "    np.set_printoptions(precision=2, suppress=True)\n",
    "\n",
    "    print(f\"\\n=== {label} ===\")\n",
    "    print(f\"Iterations: {iters}\")\n",
    "    print(f\"Optimization time: {elapsed:.6f} seconds\")\n",
    "    print(\"\\nV* (state-value function):\")\n",
    "    print(V)\n",
    "\n",
    "    print(\"\\nœÄ* (greedy policy from V*):\")\n",
    "    policy_grid = format_policy(env, policy_idx)\n",
    "    for r in range(env.n_rows):\n",
    "        print(\" \".join(f\"{cell:>2}\" for cell in policy_grid[r]))\n",
    "\n",
    "    print(\"\\nLegend:\")\n",
    "    print(\"  ‚Üí Right, ‚Üì Down, ‚Üê Left, ‚Üë Up, G Goal/Terminal, X? = Grey state with suggested move\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = Gridworld5x5()\n",
    "\n",
    "    V_star, pi_star_idx, n_iters, opt_time = value_iteration(\n",
    "        env,\n",
    "        gamma=0.9,\n",
    "        theta=1e-6,\n",
    "        max_iters=10000,\n",
    "    )\n",
    "\n",
    "    print_results(env, V_star, pi_star_idx, n_iters, opt_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c415f3",
   "metadata": {},
   "source": [
    "`Task 2: Value Iteration Variations`\n",
    "\n",
    "Implement the following variation of value iteration. Confirm that it reaches the same optimal statevalue function and policy.\n",
    "\n",
    "1. In-Place Value Iteration: Use a single array to store the state values. This means that you\n",
    "update the value of a state and immediately use that updated value in the subsequent updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "31bf4b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse Gridworld5x5, format_policy, print_results from Task 1 code\n",
    "\n",
    "\n",
    "def value_iteration_in_place(\n",
    "    env,\n",
    "    gamma: float = 0.9,\n",
    "    theta: float = 1e-6,\n",
    "    max_iters: int = 10000,\n",
    ") -> Tuple[np.ndarray, np.ndarray, int, float]:\n",
    "    \"\"\"\n",
    "    In-Place Value Iteration (Gauss‚ÄìSeidel style):\n",
    "        V(s) <- max_a [ R(s) + gamma * V(s') ]\n",
    "    Updates are written back into V immediately within the same sweep.\n",
    "\n",
    "    Returns:\n",
    "      V* grid,\n",
    "      greedy policy indices,\n",
    "      iterations used,\n",
    "      optimization time (seconds).\n",
    "    \"\"\"\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    V = np.zeros((env.n_rows, env.n_cols), dtype=float)\n",
    "    action_names = [name for name, _ in env.actions]\n",
    "\n",
    "    it = 0\n",
    "    for it in range(1, max_iters + 1):\n",
    "        delta = 0.0\n",
    "\n",
    "        # sweep through states (row-major order)\n",
    "        for r in range(env.n_rows):\n",
    "            for c in range(env.n_cols):\n",
    "                s = (r, c)\n",
    "\n",
    "                # Terminal: keep at terminal reward (clear for your V* table)\n",
    "                if env.is_terminal(s):\n",
    "                    old = V[r, c]\n",
    "                    V[r, c] = env.reward(s)\n",
    "                    delta = max(delta, abs(V[r, c] - old))\n",
    "                    continue\n",
    "\n",
    "                old = V[r, c]\n",
    "\n",
    "                best = -float(\"inf\")\n",
    "                for a in action_names:\n",
    "                    s_next = env.step(s, a)\n",
    "                    rr = env.reward(s)  # reward depends on current state\n",
    "                    candidate = rr + gamma * V[s_next[0], s_next[1]]  # uses UPDATED V when available\n",
    "                    if candidate > best:\n",
    "                        best = candidate\n",
    "\n",
    "                V[r, c] = best\n",
    "                delta = max(delta, abs(V[r, c] - old))\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    # Derive greedy policy œÄ* from V*\n",
    "    policy_idx = np.full((env.n_rows, env.n_cols), fill_value=-1, dtype=int)\n",
    "\n",
    "    for r in range(env.n_rows):\n",
    "        for c in range(env.n_cols):\n",
    "            s = (r, c)\n",
    "            if env.is_terminal(s):\n",
    "                policy_idx[r, c] = -1\n",
    "                continue\n",
    "\n",
    "            best_a = 0\n",
    "            best_val = -float(\"inf\")\n",
    "            for i, a in enumerate(action_names):\n",
    "                s_next = env.step(s, a)\n",
    "                rr = env.reward(s)\n",
    "                val = rr + gamma * V[s_next[0], s_next[1]]\n",
    "                if val > best_val:\n",
    "                    best_val = val\n",
    "                    best_a = i\n",
    "\n",
    "            policy_idx[r, c] = best_a\n",
    "\n",
    "    elapsed = time.perf_counter() - start\n",
    "    return V, policy_idx, it, elapsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9b0ec33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Task 1 vs Task 2 Comparison ===\n",
      "Task 1 (two-array):   iterations=10, time=0.001406s\n",
      "Task 2 (in-place):    iterations=10, time=0.001909s\n",
      "Same V* (tol 1e-4)?   True\n",
      "Same œÄ* exactly?      True\n",
      "\n",
      "=== Task 1 ‚Äî Standard Value Iteration (Two-Array) ===\n",
      "Iterations: 10\n",
      "Optimization time: 0.001406 seconds\n",
      "\n",
      "V* (state-value function):\n",
      "[[-1.39 -0.43  0.63  1.81 -0.88]\n",
      " [-0.43  0.63 -2.19  3.12  4.58]\n",
      " [ 0.63  1.81  3.12  4.58  6.2 ]\n",
      " [-2.19  3.12  4.58  6.2   8.  ]\n",
      " [ 3.12  4.58  6.2   8.   10.  ]]\n",
      "\n",
      "œÄ* (greedy policy from V*):\n",
      " ‚Üí  ‚Üí  ‚Üí  ‚Üì X‚Üì\n",
      " ‚Üí  ‚Üì X‚Üí  ‚Üí  ‚Üì\n",
      " ‚Üí  ‚Üí  ‚Üí  ‚Üí  ‚Üì\n",
      "X‚Üí  ‚Üí  ‚Üí  ‚Üí  ‚Üì\n",
      " ‚Üí  ‚Üí  ‚Üí  ‚Üí  G\n",
      "\n",
      "Legend:\n",
      "  ‚Üí Right, ‚Üì Down, ‚Üê Left, ‚Üë Up, G Goal/Terminal, X? = Grey state with suggested move\n",
      "\n",
      "=== Task 2 ‚Äî In-Place Value Iteration ===\n",
      "Iterations: 10\n",
      "Optimization time: 0.001909 seconds\n",
      "\n",
      "V* (state-value function):\n",
      "[[-1.39 -0.43  0.63  1.81 -0.88]\n",
      " [-0.43  0.63 -2.19  3.12  4.58]\n",
      " [ 0.63  1.81  3.12  4.58  6.2 ]\n",
      " [-2.19  3.12  4.58  6.2   8.  ]\n",
      " [ 3.12  4.58  6.2   8.   10.  ]]\n",
      "\n",
      "œÄ* (greedy policy from V*):\n",
      " ‚Üí  ‚Üí  ‚Üí  ‚Üì X‚Üì\n",
      " ‚Üí  ‚Üì X‚Üí  ‚Üí  ‚Üì\n",
      " ‚Üí  ‚Üí  ‚Üí  ‚Üí  ‚Üì\n",
      "X‚Üí  ‚Üí  ‚Üí  ‚Üí  ‚Üì\n",
      " ‚Üí  ‚Üí  ‚Üí  ‚Üí  G\n",
      "\n",
      "Legend:\n",
      "  ‚Üí Right, ‚Üì Down, ‚Üê Left, ‚Üë Up, G Goal/Terminal, X? = Grey state with suggested move\n"
     ]
    }
   ],
   "source": [
    "def compare_task1_vs_task2(env, gamma=0.9, theta=1e-6, max_iters=10000):\n",
    "    V1, pi1, it1, t1 = value_iteration(env, gamma=gamma, theta=theta, max_iters=max_iters)\n",
    "    V2, pi2, it2, t2 = value_iteration_in_place(env, gamma=gamma, theta=theta, max_iters=max_iters)\n",
    "\n",
    "    # Check they converge to the same V* (within tolerance)\n",
    "    same_values = np.allclose(V1, V2, atol=1e-4)\n",
    "    same_policy = np.array_equal(pi1, pi2)\n",
    "\n",
    "    print(\"\\n=== Task 1 vs Task 2 Comparison ===\")\n",
    "    print(f\"Task 1 (two-array):   iterations={it1}, time={t1:.6f}s\")\n",
    "    print(f\"Task 2 (in-place):    iterations={it2}, time={t2:.6f}s\")\n",
    "    print(f\"Same V* (tol 1e-4)?   {same_values}\")\n",
    "    print(f\"Same œÄ* exactly?      {same_policy}\")\n",
    "\n",
    "    return (V1, pi1, it1, t1), (V2, pi2, it2, t2)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = Gridworld5x5()\n",
    "    (V_star_1, pi_1, it1, t1), (V_star_2, pi_2, it2, t2) = compare_task1_vs_task2(env)\n",
    "\n",
    "    # print(\"\\n--- Task 2 Results (In-Place) ---\")\n",
    "    # print_results(env, V_star_2, pi_2, it2, t2)\n",
    "    print_results(env, V_star_1, pi_1, it1, t1, label=\"Task 1 ‚Äî Standard Value Iteration (Two-Array)\")\n",
    "    print_results(env, V_star_2, pi_2, it2, t2, label=\"Task 2 ‚Äî In-Place Value Iteration\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fb21ad",
   "metadata": {},
   "source": [
    "`Reflection`\n",
    "\n",
    "| Metric                             | Task 1 (Two-Array) | Task 2 (In-Place) |\n",
    "| ---------------------------------- | --------------------- | -------------------- |\n",
    "| Iterations (Sweeps)                | 10                    | 10                   |\n",
    "| Optimization Time (seconds)        | 0.009354 s            | 0.008264 s           |\n",
    "| Same Optimal Value Function (V^*)? | ‚Äî                     | Yes                  |\n",
    "| Same Optimal Policy ( \\pi^* )?     | ‚Äî                     | Yes                  |\n",
    "\n",
    "\n",
    "In Task 1 and Task 2, we implemented two versions of Value Iteration to solve the 5√ó5 Gridworld. Both approaches successfully converged to the same optimal state-value function ùëâ and the same optimal policy ùúã. The values increased as states got closer to the goal at (4,4), and states near the grey penalty cells had lower values. This confirms that the algorithm correctly learned to prefer paths that maximize long-term reward while avoiding unfavorable states.\n",
    "\n",
    "When comparing performance, both methods converged in 10 iterations. However, the in-place Value Iteration ran slightly faster (0.008264 seconds) than the two-array version (0.009354 seconds). This small difference occurs because the in-place method updates values immediately and can use newly improved values within the same sweep. Even though both methods have the same theoretical complexity, the in-place version can be more efficient in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239627e0",
   "metadata": {},
   "source": [
    "### PROBLEM 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3951b2f2",
   "metadata": {},
   "source": [
    "`Task`\n",
    "\n",
    "Implement the off-policy Monte Carlo with Importance sampling algorithm to estimate the value\n",
    "function for the given gridworld. Use a fixed behavior policy b(a|s) (e.g., a random policy) to generate\n",
    "episodes and a greedy target policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cb5090",
   "metadata": {},
   "outputs": [],
   "source": [
    "State = Tuple[int, int]\n",
    "\n",
    "# Environment is the same as Problem 3\n",
    "\n",
    "class Gridworld5x5:\n",
    "    def __init__(self):\n",
    "        self.n_rows = 5\n",
    "        self.n_cols = 5\n",
    "        self.goal: State = (4, 4)\n",
    "        self.grey_states = {(1, 2), (3, 0), (0, 4)}\n",
    "\n",
    "        # action index -> (name, (dr, dc))\n",
    "        self.actions = [\n",
    "            (\"R\", (0, 1)),\n",
    "            (\"D\", (1, 0)),\n",
    "            (\"L\", (0, -1)),\n",
    "            (\"U\", (-1, 0)),\n",
    "        ]\n",
    "\n",
    "    def is_terminal(self, s: State) -> bool:\n",
    "        return s == self.goal\n",
    "\n",
    "    def reward(self, s: State) -> float:\n",
    "        # reward depends on current state (as per assignment)\n",
    "        if s == self.goal:\n",
    "            return 10.0\n",
    "        if s in self.grey_states:\n",
    "            return -5.0\n",
    "        return -1.0\n",
    "\n",
    "    def step(self, s: State, a_idx: int) -> State:\n",
    "        # terminal is absorbing\n",
    "        if self.is_terminal(s):\n",
    "            return s\n",
    "\n",
    "        dr, dc = self.actions[a_idx][1]\n",
    "        r, c = s\n",
    "        nr, nc = r + dr, c + dc\n",
    "\n",
    "        # wall -> stay\n",
    "        if nr < 0 or nr >= self.n_rows or nc < 0 or nc >= self.n_cols:\n",
    "            return s\n",
    "        return (nr, nc)\n",
    "\n",
    "    def all_states(self) -> List[State]:\n",
    "        return [(r, c) for r in range(self.n_rows) for c in range(self.n_cols)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4185de20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_to_arrows(env: Gridworld5x5, pi: np.ndarray) -> List[List[str]]:\n",
    "    # pi stores action index per state; -1 for terminal\n",
    "    arrows = {0: \"‚Üí\", 1: \"‚Üì\", 2: \"‚Üê\", 3: \"‚Üë\", -1: \"G\"}\n",
    "    grid = []\n",
    "    for r in range(env.n_rows):\n",
    "        row = []\n",
    "        for c in range(env.n_cols):\n",
    "            s = (r, c)\n",
    "            if env.is_terminal(s):\n",
    "                row.append(\"G\")\n",
    "            elif s in env.grey_states:\n",
    "                row.append(\"X\" + arrows[int(pi[r, c])])\n",
    "            else:\n",
    "                row.append(arrows[int(pi[r, c])])\n",
    "        grid.append(row)\n",
    "    return grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "844d03d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_value_and_policy(env: Gridworld5x5, V: np.ndarray, pi: np.ndarray, title: str):\n",
    "    np.set_printoptions(precision=2, suppress=True)\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    print(\"\\nEstimated V (5x5):\")\n",
    "    print(V)\n",
    "\n",
    "    print(\"\\nGreedy policy œÄ (arrows):\")\n",
    "    grid = policy_to_arrows(env, pi)\n",
    "    for r in range(env.n_rows):\n",
    "        print(\" \".join(f\"{cell:>2}\" for cell in grid[r]))\n",
    "\n",
    "    print(\"\\nLegend: ‚Üí Right, ‚Üì Down, ‚Üê Left, ‚Üë Up, G Goal, X? Grey state\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3a304364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Behavior policy b(a|s): fixed uniform random\n",
    "\n",
    "def behavior_policy_probs(n_actions: int) -> np.ndarray:\n",
    "    return np.ones(n_actions) / n_actions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fe2cde89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an episode under behavior policy b\n",
    "\n",
    "def generate_episode(\n",
    "    env: Gridworld5x5,\n",
    "    start_state: State,\n",
    "    gamma: float,\n",
    "    rng: np.random.Generator,\n",
    "    max_steps: int = 200,\n",
    ") -> List[Tuple[State, int, float]]:\n",
    "    \"\"\"\n",
    "    Episode = list of (state, action, reward).\n",
    "    Reward here is R(s_t) based on current state, matching assignment.\n",
    "    \"\"\"\n",
    "    episode = []\n",
    "    s = start_state\n",
    "\n",
    "    b_probs = behavior_policy_probs(len(env.actions))\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        # choose action from behavior policy\n",
    "        a = rng.choice(len(env.actions), p=b_probs)\n",
    "        r = env.reward(s)\n",
    "        episode.append((s, a, r))\n",
    "\n",
    "        s = env.step(s, a)\n",
    "        if env.is_terminal(s):\n",
    "            # include terminal state's reward once (optional but consistent):\n",
    "            # In your Problem 3 you set V(goal)=10. Here we can add final transition reward.\n",
    "            # We'll append terminal reward as a final step with dummy action = -1.\n",
    "            episode.append((s, -1, env.reward(s)))\n",
    "            break\n",
    "\n",
    "    return episode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7e33c436",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Off-policy Monte Carlo Control (Weighted Importance Sampling)\n",
    "\n",
    "def off_policy_mc_control_weighted_is(\n",
    "    env: Gridworld5x5,\n",
    "    num_episodes: int = 20000,\n",
    "    gamma: float = 0.9,\n",
    "    max_steps: int = 200,\n",
    "    seed: int = 7,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Learns Q(s,a) and a greedy target policy œÄ from episodes generated by behavior policy b.\n",
    "    Uses Weighted Importance Sampling to update Q.\n",
    "\n",
    "    Returns:\n",
    "      V_est (5x5),\n",
    "      pi_greedy (5x5 action indices; -1 at terminal),\n",
    "      Q (5x5x|A|),\n",
    "      elapsed time.\n",
    "    \"\"\"\n",
    "    start = time.perf_counter()\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    nA = len(env.actions)\n",
    "\n",
    "    # Q and C (cumulative weights) arrays\n",
    "    Q = np.zeros((env.n_rows, env.n_cols, nA), dtype=float)\n",
    "    C = np.zeros_like(Q)\n",
    "\n",
    "    # target policy œÄ initialized arbitrarily (e.g., all \"Right\")\n",
    "    pi = np.zeros((env.n_rows, env.n_cols), dtype=int)\n",
    "    pi[env.goal[0], env.goal[1]] = -1  # terminal marker\n",
    "\n",
    "    b_probs = behavior_policy_probs(nA)\n",
    "\n",
    "    # sample starting states uniformly (excluding terminal)\n",
    "    non_terminal_states = [s for s in env.all_states() if not env.is_terminal(s)]\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        s0 = non_terminal_states[rng.integers(len(non_terminal_states))]\n",
    "        episode = generate_episode(env, s0, gamma, rng, max_steps=max_steps)\n",
    "\n",
    "        G = 0.0\n",
    "        W = 1.0\n",
    "\n",
    "        # Work backward through episode (skip last terminal dummy action step)\n",
    "        for (s, a, r) in reversed(episode):\n",
    "            if a == -1:\n",
    "                # terminal dummy step: just accumulate reward and continue\n",
    "                G = gamma * G + r\n",
    "                continue\n",
    "\n",
    "            r0, c0 = s\n",
    "            G = gamma * G + r  # return from this time step\n",
    "\n",
    "            # Weighted IS update\n",
    "            C[r0, c0, a] += W\n",
    "            Q[r0, c0, a] += (W / C[r0, c0, a]) * (G - Q[r0, c0, a])\n",
    "\n",
    "            # improve target policy greedily w.r.t Q\n",
    "            pi[r0, c0] = int(np.argmax(Q[r0, c0, :]))\n",
    "\n",
    "            # If behavior took an action that target would NOT take, importance weight becomes 0 thereafter\n",
    "            if a != pi[r0, c0]:\n",
    "                break\n",
    "\n",
    "            # update importance weight: W *= œÄ(a|s)/b(a|s)\n",
    "            # œÄ is deterministic greedy => œÄ(a|s)=1 for chosen greedy action\n",
    "            W *= 1.0 / b_probs[a]\n",
    "\n",
    "    elapsed = time.perf_counter() - start\n",
    "\n",
    "    # compute V from Q under greedy œÄ\n",
    "    V = np.zeros((env.n_rows, env.n_cols), dtype=float)\n",
    "    for r in range(env.n_rows):\n",
    "        for c in range(env.n_cols):\n",
    "            s = (r, c)\n",
    "            if env.is_terminal(s):\n",
    "                V[r, c] = env.reward(s)\n",
    "            else:\n",
    "                V[r, c] = np.max(Q[r, c, :])\n",
    "\n",
    "    return V, pi, Q, elapsed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0ed8d3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  compare MC V to Value Iteration V*\n",
    "def mse(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    return float(np.mean((a - b) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b63d1fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Problem 4 ‚Äî Off-policy MC (Weighted IS) ===\n",
      "\n",
      "Estimated V (5x5):\n",
      "[[-1.39 -0.44  0.62  1.8  -0.9 ]\n",
      " [-0.44  0.62 -2.2   3.11  4.57]\n",
      " [ 0.62  1.8   3.1   4.56  6.19]\n",
      " [-2.2   3.11  4.56  6.19  8.  ]\n",
      " [ 3.1   4.56  6.19  8.   10.  ]]\n",
      "\n",
      "Greedy policy œÄ (arrows):\n",
      " ‚Üì  ‚Üì  ‚Üí  ‚Üì X‚Üì\n",
      " ‚Üí  ‚Üì X‚Üì  ‚Üí  ‚Üì\n",
      " ‚Üí  ‚Üì  ‚Üí  ‚Üì  ‚Üì\n",
      "X‚Üí  ‚Üí  ‚Üì  ‚Üì  ‚Üì\n",
      " ‚Üí  ‚Üí  ‚Üí  ‚Üí  G\n",
      "\n",
      "Legend: ‚Üí Right, ‚Üì Down, ‚Üê Left, ‚Üë Up, G Goal, X? Grey state\n",
      "\n",
      "Episodes: 20000\n",
      "Optimization time: 42.332518 seconds\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = Gridworld5x5()\n",
    "\n",
    "    # Run off-policy MC control\n",
    "    V_mc, pi_mc, Q_mc, t_mc = off_policy_mc_control_weighted_is(\n",
    "        env,\n",
    "        num_episodes=20000,   # increase for tighter match to VI\n",
    "        gamma=0.9,\n",
    "        max_steps=200,\n",
    "        seed=7,\n",
    "    )\n",
    "\n",
    "    print_value_and_policy(env, V_mc, pi_mc, title=\"Problem 4 ‚Äî Off-policy MC (Weighted IS)\")\n",
    "\n",
    "    print(f\"\\nEpisodes: 20000\")\n",
    "    print(f\"Optimization time: {t_mc:.6f} seconds\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5add72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE vs Value Iteration: 0.0001759747569171575\n"
     ]
    }
   ],
   "source": [
    "# the v from problem 3 (value iteration) is V_star_2\n",
    "V_vi = V_star_2\n",
    "print(\"MSE vs Value Iteration:\", mse(V_mc, V_vi))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
